{"paragraphs":[{"text":"%md # HDFS Locality Test\n\n**As described on https://github.com/apache-spark-on-k8s/kubernetes-HDFS**\n\nPut a local file on HDFS and set replication factor to e.g. 3.\n\n```\nhdfs dfs -cp file:/etc/hosts /hosts\nhdfs dfs -ls /hosts\nhdfs dfs -setrep 3 /hosts\n```","user":"anonymous","dateUpdated":"2017-11-24T15:53:42+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>HDFS Locality Test</h1>\n<p>**As described on <a href=\"https://github.com/apache-spark-on-k8s/kubernetes-HDFS**\">https://github.com/apache-spark-on-k8s/kubernetes-HDFS**</a></p>\n<p>Put a local file on HDFS and set replication factor to e.g. 3.</p>\n<pre><code>hdfs dfs -cp file:/etc/hosts /hosts\nhdfs dfs -ls /hosts\nhdfs dfs -setrep 3 /hosts\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1511287369656_1797589836","id":"20171121-162613_1871875075","dateCreated":"2017-11-21T18:02:49+0000","dateStarted":"2017-11-24T15:53:42+0000","dateFinished":"2017-11-24T15:53:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:290"},{"text":"%sh \n# hdfs dfsadmin -safemode leave\nhdfs dfs -cp file:/etc/hosts /hosts\nhdfs dfs -ls /hosts\nhdfs dfs -setrep 3 /hosts","user":"anonymous","dateUpdated":"2017-11-24T14:29:24+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false},"colWidth":6,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"-rw-r--r--   3 root supergroup        252 2017-11-24 14:29 /hosts\nReplication 3 set: /hosts\n"}]},"apps":[],"jobName":"paragraph_1511287369659_834467328","id":"20171121-162458_1062694110","dateCreated":"2017-11-21T18:02:49+0000","dateStarted":"2017-11-24T14:29:25+0000","dateFinished":"2017-11-24T14:29:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:291"},{"text":"%sh\n# Get the local node host IP address\necho \"Pod name: \"$HOSTNAME\necho \"Host IP: \"$(kubectl get pods $HOSTNAME -o jsonpath=\"{.status.hostIP}\")\necho \"Pod IP: \"$(kubectl get pods $HOSTNAME -o jsonpath=\"{.status.podIP}\")","user":"anonymous","dateUpdated":"2017-11-24T15:53:23+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false},"colWidth":6,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Pod name: zeppelin-k8s-hdfs-locality-zeppelin-76b6cdd799-mw9q6\nHost IP: 10.0.3.44\nPod IP: 192.168.155.137\n"}]},"apps":[],"jobName":"paragraph_1511287369659_1573570675","id":"20171121-163112_198984048","dateCreated":"2017-11-21T18:02:49+0000","dateStarted":"2017-11-24T15:53:23+0000","dateFinished":"2017-11-24T15:53:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:292"},{"text":"%md\n\nRun the following `hdfs cat` command and from the debug messages, see which datanode is being used and make sure it is your local datanode (use kubectl to get the IP addresses).\n\n```\n$ hdfs --loglevel DEBUG dfs -cat /hosts\n...\n17/04/24 20:51:28 DEBUG hdfs.DFSClient: Connecting to datanode 10.128.0.4:50010\n...\n```\n\nIf not, you should check if your local datanode is even in the list from the debug messsages above. If it is not, then this is because step (3) did not finish yet. Wait more. (You can use a smaller cluster for this test if that is possible)\n\n```\n17/04/24 20:51:28 DEBUG hdfs.DFSClient: newInfo = LocatedBlocks{\nfileLength=199\n  underConstruction=false\n    blocks=[LocatedBlock{BP-347555225-10.128.0.2-1493066928989:blk_1073741825_1001;\n    getBlockSize()=199; corrupt=false; offset=0;\n    locs=[DatanodeInfoWithStorage[10.128.0.4:50010,DS-d2de9d29-6962-4435-a4b4-aadf4ea67e46,DISK],\n    DatanodeInfoWithStorage[10.128.0.3:50010,DS-0728ffcf-f400-4919-86bf-af0f9af36685,DISK],\n    DatanodeInfoWithStorage[10.128.0.2:50010,DS-3a881114-af08-47de-89cf-37dec051c5c2,DISK]]}]\n      lastLocatedBlock=LocatedBlock{BP-347555225-10.128.0.2-1493066928989:blk_1073741825_1001;\n```\n\nRepeat the `hdfs cat` command multiple times. Check if the same datanode is being consistently used.","user":"anonymous","dateUpdated":"2017-11-24T15:53:54+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Run the following <code>hdfs cat</code> command and from the debug messages, see which datanode is being used and make sure it is your local datanode (use kubectl to get the IP addresses).</p>\n<pre><code>$ hdfs --loglevel DEBUG dfs -cat /hosts\n...\n17/04/24 20:51:28 DEBUG hdfs.DFSClient: Connecting to datanode 10.128.0.4:50010\n...\n</code></pre>\n<p>If not, you should check if your local datanode is even in the list from the debug messsages above. If it is not, then this is because step (3) did not finish yet. Wait more. (You can use a smaller cluster for this test if that is possible)</p>\n<pre><code>17/04/24 20:51:28 DEBUG hdfs.DFSClient: newInfo = LocatedBlocks{\nfileLength=199\n  underConstruction=false\n    blocks=[LocatedBlock{BP-347555225-10.128.0.2-1493066928989:blk_1073741825_1001;\n    getBlockSize()=199; corrupt=false; offset=0;\n    locs=[DatanodeInfoWithStorage[10.128.0.4:50010,DS-d2de9d29-6962-4435-a4b4-aadf4ea67e46,DISK],\n    DatanodeInfoWithStorage[10.128.0.3:50010,DS-0728ffcf-f400-4919-86bf-af0f9af36685,DISK],\n    DatanodeInfoWithStorage[10.128.0.2:50010,DS-3a881114-af08-47de-89cf-37dec051c5c2,DISK]]}]\n      lastLocatedBlock=LocatedBlock{BP-347555225-10.128.0.2-1493066928989:blk_1073741825_1001;\n</code></pre>\n<p>Repeat the <code>hdfs cat</code> command multiple times. Check if the same datanode is being consistently used.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1511538812532_-733194","id":"20171124-155332_1319366629","dateCreated":"2017-11-24T15:53:32+0000","dateStarted":"2017-11-24T15:53:54+0000","dateFinished":"2017-11-24T15:53:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:293"},{"text":"%sh hdfs --loglevel DEBUG dfs -cat /hosts","user":"anonymous","dateUpdated":"2017-11-24T15:53:20+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"17/11/24 15:53:20 DEBUG util.Shell: setsid exited with exit code 0\n17/11/24 15:53:20 DEBUG conf.Configuration: parsing URL jar:file:/opt/hadoop/share/hadoop/common/hadoop-common-2.7.3.jar!/core-default.xml\n17/11/24 15:53:20 DEBUG conf.Configuration: parsing input stream sun.net.www.protocol.jar.JarURLConnection$JarURLInputStream@7e0b0338\n17/11/24 15:53:20 DEBUG conf.Configuration: parsing URL file:/etc/hdfs-locality/conf/core-site.xml\n17/11/24 15:53:20 DEBUG conf.Configuration: parsing input stream java.io.BufferedInputStream@480bdb19\n17/11/24 15:53:21 DEBUG lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Rate of successful kerberos logins and latency (milliseconds)])\n17/11/24 15:53:21 DEBUG lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[Rate of failed kerberos logins and latency (milliseconds)])\n17/11/24 15:53:21 DEBUG lib.MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=, always=false, sampleName=Ops, type=DEFAULT, valueName=Time, value=[GetGroups])\n17/11/24 15:53:21 DEBUG impl.MetricsSystemImpl: UgiMetrics, User and group related metrics\n17/11/24 15:53:21 DEBUG util.KerberosName: Kerberos krb5 configuration not found, setting default realm to empty\n17/11/24 15:53:21 DEBUG security.Groups:  Creating new Groups object\n17/11/24 15:53:21 DEBUG util.NativeCodeLoader: Trying to load the custom-built native-hadoop library...\n17/11/24 15:53:21 DEBUG util.NativeCodeLoader: Loaded the native-hadoop library\n17/11/24 15:53:21 DEBUG security.JniBasedUnixGroupsMapping: Using JniBasedUnixGroupsMapping for Group resolution\n17/11/24 15:53:21 DEBUG security.JniBasedUnixGroupsMappingWithFallback: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMapping\n17/11/24 15:53:21 DEBUG security.Groups: Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000\n17/11/24 15:53:21 DEBUG security.UserGroupInformation: hadoop login\n17/11/24 15:53:21 DEBUG security.UserGroupInformation: hadoop login commit\n17/11/24 15:53:21 DEBUG security.UserGroupInformation: using local user:UnixPrincipal: root\n17/11/24 15:53:21 DEBUG security.UserGroupInformation: Using user: \"UnixPrincipal: root\" with name root\n17/11/24 15:53:21 DEBUG security.UserGroupInformation: User entry: \"root\"\n17/11/24 15:53:21 DEBUG security.UserGroupInformation: UGI loginUser:root (auth:SIMPLE)\n17/11/24 15:53:21 DEBUG hdfs.BlockReaderLocal: dfs.client.use.legacy.blockreader.local = false\n17/11/24 15:53:21 DEBUG hdfs.BlockReaderLocal: dfs.client.read.shortcircuit = false\n17/11/24 15:53:21 DEBUG hdfs.BlockReaderLocal: dfs.client.domain.socket.data.traffic = false\n17/11/24 15:53:21 DEBUG hdfs.BlockReaderLocal: dfs.domain.socket.path = \n17/11/24 15:53:21 DEBUG retry.RetryUtils: multipleLinearRandomRetry = null\n17/11/24 15:53:21 DEBUG ipc.Server: rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcRequestWrapper, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@229c6181\n17/11/24 15:53:21 DEBUG ipc.Client: getting client out of cache: org.apache.hadoop.ipc.Client@6d4d66d2\n17/11/24 15:53:22 DEBUG unix.DomainSocketWatcher: org.apache.hadoop.net.unix.DomainSocketWatcher$2@5aa663f5: starting with interruptCheckPeriodMs = 60000\n17/11/24 15:53:22 DEBUG util.PerformanceAdvisory: Both short-circuit local reads and UNIX domain socket are disabled.\n17/11/24 15:53:22 DEBUG sasl.DataTransferSaslUtil: DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection\n17/11/24 15:53:22 DEBUG ipc.Client: The ping interval is 60000 ms.\n17/11/24 15:53:22 DEBUG ipc.Client: Connecting to hdfs-namenode-0.hdfs-namenode.default.svc.cluster.local/10.0.3.74:8020\n17/11/24 15:53:22 DEBUG ipc.Client: IPC Client (2014461570) connection to hdfs-namenode-0.hdfs-namenode.default.svc.cluster.local/10.0.3.74:8020 from root: starting, having connections 1\n17/11/24 15:53:22 DEBUG ipc.Client: IPC Client (2014461570) connection to hdfs-namenode-0.hdfs-namenode.default.svc.cluster.local/10.0.3.74:8020 from root sending #0\n17/11/24 15:53:22 DEBUG ipc.Client: IPC Client (2014461570) connection to hdfs-namenode-0.hdfs-namenode.default.svc.cluster.local/10.0.3.74:8020 from root got value #0\n17/11/24 15:53:22 DEBUG ipc.ProtobufRpcEngine: Call: getFileInfo took 40ms\n17/11/24 15:53:22 DEBUG ipc.Client: IPC Client (2014461570) connection to hdfs-namenode-0.hdfs-namenode.default.svc.cluster.local/10.0.3.74:8020 from root sending #1\n17/11/24 15:53:22 DEBUG ipc.Client: IPC Client (2014461570) connection to hdfs-namenode-0.hdfs-namenode.default.svc.cluster.local/10.0.3.74:8020 from root got value #1\n17/11/24 15:53:22 DEBUG ipc.ProtobufRpcEngine: Call: getBlockLocations took 2ms\n17/11/24 15:53:22 DEBUG hdfs.DFSClient: newInfo = LocatedBlocks{\n  fileLength=252\n  underConstruction=false\n  blocks=[LocatedBlock{BP-630686692-10.0.3.74-1511532529906:blk_1073741825_1001; getBlockSize()=252; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[10.0.3.44:50010,DS-2db7a503-0fcc-4985-b61d-9e4a373dcc79,DISK], DatanodeInfoWithStorage[10.0.2.115:50010,DS-aac8e655-dfb7-47de-83d6-b7cea294d922,DISK], DatanodeInfoWithStorage[10.0.2.246:50010,DS-80598f0a-f64a-43c9-92f6-e6210a3565b9,DISK]]}]\n  lastLocatedBlock=LocatedBlock{BP-630686692-10.0.3.74-1511532529906:blk_1073741825_1001; getBlockSize()=252; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[10.0.3.44:50010,DS-2db7a503-0fcc-4985-b61d-9e4a373dcc79,DISK], DatanodeInfoWithStorage[10.0.2.115:50010,DS-aac8e655-dfb7-47de-83d6-b7cea294d922,DISK], DatanodeInfoWithStorage[10.0.2.246:50010,DS-80598f0a-f64a-43c9-92f6-e6210a3565b9,DISK]]}\n  isLastBlockComplete=true}\n17/11/24 15:53:22 DEBUG hdfs.DFSClient: Connecting to datanode 10.0.3.44:50010\n17/11/24 15:53:22 DEBUG ipc.Client: IPC Client (2014461570) connection to hdfs-namenode-0.hdfs-namenode.default.svc.cluster.local/10.0.3.74:8020 from root sending #2\n17/11/24 15:53:22 DEBUG ipc.Client: IPC Client (2014461570) connection to hdfs-namenode-0.hdfs-namenode.default.svc.cluster.local/10.0.3.74:8020 from root got value #2\n17/11/24 15:53:22 DEBUG ipc.ProtobufRpcEngine: Call: getServerDefaults took 1ms\n17/11/24 15:53:22 DEBUG sasl.SaslDataTransferClient: SASL client skipping handshake in unsecured configuration for addr = /10.0.3.44, datanodeId = DatanodeInfoWithStorage[10.0.3.44:50010,DS-2db7a503-0fcc-4985-b61d-9e4a373dcc79,DISK]\n# Kubernetes-managed hosts file.\n127.0.0.1\tlocalhost\n::1\tlocalhost ip6-localhost ip6-loopback\nfe00::0\tip6-localnet\nfe00::0\tip6-mcastprefix\nfe00::1\tip6-allnodes\nfe00::2\tip6-allrouters\n192.168.155.137\tzeppelin-k8s-hdfs-locality-zeppelin-76b6cdd799-mw9q6\n17/11/24 15:53:22 DEBUG ipc.Client: stopping client from cache: org.apache.hadoop.ipc.Client@6d4d66d2\n17/11/24 15:53:22 DEBUG ipc.Client: removing client from cache: org.apache.hadoop.ipc.Client@6d4d66d2\n17/11/24 15:53:22 DEBUG ipc.Client: stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@6d4d66d2\n17/11/24 15:53:22 DEBUG ipc.Client: Stopping client\n17/11/24 15:53:22 DEBUG ipc.Client: IPC Client (2014461570) connection to hdfs-namenode-0.hdfs-namenode.default.svc.cluster.local/10.0.3.74:8020 from root: closed\n17/11/24 15:53:22 DEBUG ipc.Client: IPC Client (2014461570) connection to hdfs-namenode-0.hdfs-namenode.default.svc.cluster.local/10.0.3.74:8020 from root: stopped, remaining connections 0\n"}]},"apps":[],"jobName":"paragraph_1511287369660_208453910","id":"20171121-162530_1287902751","dateCreated":"2017-11-21T18:02:49+0000","dateStarted":"2017-11-24T15:53:20+0000","dateFinished":"2017-11-24T15:53:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:294"},{"text":"","user":"anonymous","dateUpdated":"2017-11-24T15:40:20+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511537519156_-732700265","id":"20171124-153159_2081524587","dateCreated":"2017-11-24T15:31:59+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:295"},{"text":"List(1,2,3)\n  .toDS.map(_*2)\n  .repartition(2).write.mode(\"overwrite\").parquet(\"/tmp/ds\")\nspark.read.parquet(\"/tmp/ds\").show","user":"anonymous","dateUpdated":"2017-11-24T15:32:20+0000","config":{"colWidth":6,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----+\n|value|\n+-----+\n|    4|\n|    6|\n|    2|\n+-----+\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://192.168.155.137:4040/jobs/job/?id=6","http://192.168.155.137:4040/jobs/job/?id=7","http://192.168.155.137:4040/jobs/job/?id=8","http://192.168.155.137:4040/jobs/job/?id=9"],"interpreterSettingId":"2CBEJNFR7"}},"apps":[],"jobName":"paragraph_1511537434411_1169672882","id":"20171124-153034_298808478","dateCreated":"2017-11-24T15:30:34+0000","dateStarted":"2017-11-24T15:32:20+0000","dateFinished":"2017-11-24T15:32:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:296"},{"text":"%sh \nhdfs dfs -ls /tmp/ds","user":"anonymous","dateUpdated":"2017-11-24T15:32:03+0000","config":{"colWidth":6,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 3 items\n-rw-r--r--   3 root supergroup          0 2017-11-24 15:31 /tmp/ds/_SUCCESS\n-rw-r--r--   3 root supergroup        374 2017-11-24 15:31 /tmp/ds/part-00000-34ae6b24-96f1-46b5-b98d-c17b02487c3f-c000.snappy.parquet\n-rw-r--r--   3 root supergroup        266 2017-11-24 15:31 /tmp/ds/part-00001-34ae6b24-96f1-46b5-b98d-c17b02487c3f-c000.snappy.parquet\n"}]},"apps":[],"jobName":"paragraph_1511537451917_-220896053","id":"20171124-153051_436834673","dateCreated":"2017-11-24T15:30:51+0000","dateStarted":"2017-11-24T15:32:03+0000","dateFinished":"2017-11-24T15:32:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:297"},{"text":"%sh\n","user":"anonymous","dateUpdated":"2017-11-24T15:38:54+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511537930162_-1670872935","id":"20171124-153850_1704055728","dateCreated":"2017-11-24T15:38:50+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:298"},{"text":"%sh\nkubectl get nodes\nkubectl get pods","user":"anonymous","dateUpdated":"2017-11-24T15:51:19+0000","config":{"colWidth":6,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"NAME                                       STATUS    ROLES     AGE       VERSION\nip-10-0-0-204.us-west-2.compute.internal   Ready     master    3d        v1.8.4\nip-10-0-2-115.us-west-2.compute.internal   Ready     <none>    2h        v1.8.4\nip-10-0-2-230.us-west-2.compute.internal   Ready     <none>    2h        v1.8.4\nip-10-0-2-246.us-west-2.compute.internal   Ready     <none>    2h        v1.8.4\nip-10-0-3-43.us-west-2.compute.internal    Ready     <none>    2h        v1.8.4\nip-10-0-3-44.us-west-2.compute.internal    Ready     <none>    3h        v1.8.4\nip-10-0-3-74.us-west-2.compute.internal    Ready     <none>    3h        v1.8.4\nNAME                                                   READY     STATUS    RESTARTS   AGE\nhdfs-datanode-822rr                                    1/1       Running   0          1h\nhdfs-datanode-8ljpq                                    1/1       Running   0          1h\nhdfs-datanode-fxmrf                                    1/1       Running   0          1h\nhdfs-datanode-gfszw                                    1/1       Running   0          1h\nhdfs-datanode-vvpj4                                    1/1       Running   0          1h\nhdfs-namenode-0                                        1/1       Running   0          1h\nk8s-dashboard-kubernetes-dashboard-5c55c757f7-n798h    1/1       Running   0          3h\nspark-exec-1                                           1/1       Running   0          32m\nspark-exec-2                                           1/1       Running   0          32m\nspark-exec-3                                           1/1       Running   0          32m\nspark-k8s-resource-staging-server-7d69477f66-v2d4b     1/1       Running   0          3h\nspark-k8s-shuffle-service-fbztt                        1/1       Running   0          3h\nspark-k8s-shuffle-service-hshm7                        1/1       Running   0          3h\nspark-k8s-shuffle-service-hwx2j                        1/1       Running   0          2h\nspark-k8s-shuffle-service-jp7nf                        1/1       Running   0          2h\nspark-k8s-shuffle-service-nvdq5                        1/1       Running   0          2h\nspark-k8s-shuffle-service-qzxsc                        1/1       Running   0          2h\nzeppelin-k8s-hdfs-locality-zeppelin-76b6cdd799-mw9q6   1/1       Running   0          1h\n"}]},"apps":[],"jobName":"paragraph_1511537580346_-1844827048","id":"20171124-153300_1111561772","dateCreated":"2017-11-24T15:33:00+0000","dateStarted":"2017-11-24T15:51:19+0000","dateFinished":"2017-11-24T15:51:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:299"},{"text":"import org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.net.PodCIDRToNodeMapping\nimport collection.JavaConversions._\n\nval conf = new Configuration()\nval plugin = new PodCIDRToNodeMapping()\nplugin.setConf(conf)\n\n/**\n* Resolves a list of DNS-names/IP-addresses and returns back a list of\n* switch information (network paths). One-to-one correspondence must be \n* maintained between the elements in the lists. \n* Consider an element in the argument list - x.y.com. The switch information\n* that is returned must be a network path of the form /foo/rack, \n* where / is the root, and 'foo' is the switch where 'rack' is connected.\n* Note the hostname/ip-address is not part of the returned path.\n* The network topology of the cluster would determine the number of\n* components in the network path.\n* <p/>\n*\n* If a name cannot be resolved to a rack, the implementation\n* should return {@link NetworkTopology#DEFAULT_RACK}. This\n* is what the bundled implementations do, though it is not a formal requirement\n*\n* @param names the list of hosts to resolve (can be empty)\n* @return list of resolved network paths.\n* If <i>names</i> is empty, the returned list is also empty\n*/\nval networkPathDirs = plugin.resolve(List(\n    \"ip-10-0-0-204.us-west-2.compute.internal\",\n    \"ip-10-0-2-115.us-west-2.compute.internal\",\n    \"ip-10-0-2-230.us-west-2.compute.internal\",\n    \"ip-10-0-2-246.us-west-2.compute.internal\",\n    \"ip-10-0-3-43.us-west-2.compute.internal\",\n    \"ip-10-0-3-44.us-west-2.compute.internal\",\n    \"ip-10-0-3-74.us-west-2.compute.internal\",\n    \"10.0.3.74\",\n    \"hdfs-datanode-gfszw\",\n    \"hdfs-datanode-vvpj4\",\n    \"unknown\"\n    ))\nnetworkPathDirs.foreach(println)\n\nprintln(org.apache.hadoop.net.NetworkTopology.DEFAULT_RACK)\n\nplugin.dumpTopology\nplugin.getSwitchMap\n\nplugin.reloadCachedMappings() // Does nothing for now.","user":"anonymous","dateUpdated":"2017-11-24T16:04:32+0000","config":{"colWidth":6,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.net.PodCIDRToNodeMapping\nimport collection.JavaConversions._\nconf: org.apache.hadoop.conf.Configuration = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml\nplugin: org.apache.hadoop.net.PodCIDRToNodeMapping = org.apache.hadoop.net.PodCIDRToNodeMapping@d690d69\nnetworkPathDirs: java.util.List[String] = [/default-rack/ip-10-0-0-204, /default-rack/ip-10-0-2-115, /default-rack/ip-10-0-2-230, /default-rack/ip-10-0-2-246, /default-rack/ip-10-0-3-43, /default-rack/ip-10-0-3-44, /default-rack/ip-10-0-3-74, /default-rack/default-nodegroup, /default-rack/default-nodegroup, /default-rack/default-nodegroup, /default-rack/default-nodegroup]\n/default-rack/ip-10-0-0-204\n/default-rack/ip-10-0-2-115\n/default-rack/ip-10-0-2-230\n/default-rack/ip-10-0-2-246\n/default-rack/ip-10-0-3-43\n/default-rack/ip-10-0-3-44\n/default-rack/ip-10-0-3-74\n/default-rack/default-nodegroup\n/default-rack/default-nodegroup\n/default-rack/default-nodegroup\n/default-rack/default-nodegroup\n/default-rack\nres74: String =\nMapping: org.apache.hadoop.net.PodCIDRToNodeMapping@d690d69\nNo topology information\nres75: java.util.Map[String,String] = null\n"}]},"apps":[],"jobName":"paragraph_1511537578212_185214945","id":"20171124-153258_804701564","dateCreated":"2017-11-24T15:32:58+0000","dateStarted":"2017-11-24T16:04:32+0000","dateFinished":"2017-11-24T16:04:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:300"},{"text":"","dateUpdated":"2017-11-24T15:40:26+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511287369660_-2138342023","id":"20171121-163031_1192038626","dateCreated":"2017-11-21T18:02:49+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:301"}],"name":"3. HDFS Locality","id":"2D1UAHBVM","angularObjects":{"2CCMX5R9J:shared_process":[],"2CBEJNFR7:shared_process":[],"2CBRBCK1E:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}